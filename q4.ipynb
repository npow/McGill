{
 "metadata": {
  "name": "",
  "signature": "sha256:f7c6583b97a16dc17026c9759b9a3e52e49c26b72633e28262ac7ead666b0772"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import math\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "from functools import partial\n",
      "from scipy.special import expit as sigmoid\n",
      "from sklearn.cross_validation import *\n",
      "from sklearn.linear_model import *\n",
      "from sklearn.metrics import *\n",
      "from sklearn.metrics.pairwise import rbf_kernel\n",
      "import sys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.loadtxt('data/wpbcx.dat')\n",
      "Y = np.loadtxt('data/wpbcy.dat').astype(int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def do_cv(clf, n_folds=5):\n",
      "    #skf = KFold(X.shape[0], n_folds=n_folds)\n",
      "    skf = StratifiedKFold(Y, n_folds=n_folds)\n",
      "    D = { 'train_ll': [], 'train_acc': [], 'test_ll': [], 'test_acc': [] }\n",
      "    for train_indices, test_indices in skf:\n",
      "        X_train, X_test = X[train_indices], X[test_indices]\n",
      "        Y_train, Y_test = Y[train_indices], Y[test_indices]\n",
      "        clf.fit(X_train, Y_train)\n",
      "        test_pred = clf.predict(X_test)\n",
      "        test_pred_proba = clf.predict_proba(X_test)  \n",
      "        train_pred = clf.predict(X_train)\n",
      "        train_pred_proba = clf.predict_proba(X_train)\n",
      "        D['test_ll'].append(log_loss(Y_test, test_pred_proba))\n",
      "        D['test_acc'].append(accuracy_score(Y_test, test_pred))\n",
      "        D['train_ll'].append(log_loss(Y_train, train_pred_proba))\n",
      "        D['train_acc'].append(accuracy_score(Y_train, train_pred))        \n",
      "\n",
      "    print \"train log_loss (mean/std): %f/%f\" % (np.array(D['train_ll']).mean(), np.array(D['train_ll']).std())\n",
      "    print \"train acc (mean/std): %f/%f\" % (np.array(D['train_acc']).mean(), np.array(D['train_acc']).std())\n",
      "    print \"test log_loss (mean/std): %f/%f\" % (np.array(D['test_ll']).mean(), np.array(D['test_ll']).std())\n",
      "    print \"test acc (mean/std): %f/%f\" % (np.array(D['test_acc']).mean(), np.array(D['test_acc']).std())    \n",
      "    print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class GDA:\n",
      "    def __init__(self, full_cov):\n",
      "        self.p = [None, None]\n",
      "        self.mu = [None, None]\n",
      "        self.cov = [None, None]\n",
      "        self.full_cov = full_cov\n",
      "        \n",
      "    def fit(self, X, Y):\n",
      "        if np.unique(Y).shape[0] != 2:\n",
      "            raise \"Only binary classification supported\"\n",
      "        c = Counter(Y)\n",
      "        for k in [0, 1]:\n",
      "            self.p[k] = c[k] / Y.shape[0]\n",
      "            self.mu[k] = X[np.where(Y==k)].mean(axis=0)\n",
      "            self.cov[k] = np.sum([np.dot((X[i,:]-self.mu[k]).reshape(-1, 1), (X[i,:]-self.mu[k]).reshape(1, -1)) for i in np.where(Y==k)[0]], axis=0) / c[k]\n",
      "            if not self.full_cov:\n",
      "                self.cov[k] = np.diag(np.diag(self.cov[k]))\n",
      "                \n",
      "    def predict(self, X):\n",
      "        probas = self.predict_proba(X)\n",
      "        pred = [np.argmax(ps) for ps in probas]\n",
      "        return np.array(pred)\n",
      "\n",
      "    def predict_proba(self, X):\n",
      "        def f(x, k):\n",
      "            d = self.cov[k].shape[0]\n",
      "            A = (x-self.mu[k]).reshape(-1, 1)\n",
      "            B = np.linalg.inv(self.cov[k])\n",
      "            retval = math.exp(-0.5*np.dot(np.dot(A.T, B), A))/(((2*math.pi)**(d/2))*math.sqrt(np.linalg.det(self.cov[k])))\n",
      "            return retval * self.p[k]\n",
      "        pred = []\n",
      "        for x in X:\n",
      "            ps = np.array([f(x, 0), f(x, 1)])\n",
      "            ps /= sum(ps)\n",
      "            pred.append(ps)\n",
      "        return np.array(pred)\n",
      "\n",
      "class KLR:\n",
      "    def __init__(self, kernel=lambda v1, v2, sigma: math.exp(-np.linalg.norm(v1-v2, 2)**2/(2.*sigma**2)), alpha=0.1, l=0, epochs=1, sigma=1.0):\n",
      "        self.alpha = alpha\n",
      "        self.K = None\n",
      "        self.kernel = partial(kernel, sigma=sigma)\n",
      "        self.l = l\n",
      "        self.epochs = epochs\n",
      "        self.sigma = sigma\n",
      "        self.W = None\n",
      "        self.bias = 0\n",
      "        \n",
      "    def fit(self, X, Y):\n",
      "        #X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
      "        self.K = rbf_kernel(X, X, self.sigma)\n",
      "        self.W = np.zeros(X.shape[0])\n",
      "        for e in xrange(self.epochs):\n",
      "            #print \"epoch: \", e\n",
      "            sys.stdout.flush()\n",
      "            tmpW = np.zeros(self.W.shape)\n",
      "            tmpBias = 0\n",
      "            for z in xrange(len(self.W)):\n",
      "                grad = 0.0 \n",
      "                for i in xrange(X.shape[0]):\n",
      "                    loss = (Y[i] - sigmoid(self.bias + sum([self.W[j]*self.K[(i,j)] for j in xrange(1, len(self.W))])))\n",
      "                    loss *= self.K[(z,i)]\n",
      "                    grad += loss\n",
      "                tmpW[z] = self.W[z] - self.alpha * grad\n",
      "            grad = 0.0 \n",
      "            for i in xrange(X.shape[0]):\n",
      "                loss = (Y[i] - sigmoid(self.bias + sum([self.W[j]*self.K[(i,j)] for j in xrange(1, len(self.W))])))\n",
      "                grad += loss                \n",
      "            tmpBias = self.bias - self.alpha * grad\n",
      "            \n",
      "            self.W = np.copy(tmpW)\n",
      "            self.bias = tmpBias\n",
      "            \n",
      "    def predict(self, X):\n",
      "        probas = self.predict_proba(X)\n",
      "        pred = [np.argmax(ps) for ps in probas]\n",
      "        return np.array(pred)\n",
      "    \n",
      "    def predict_proba(self, X):\n",
      "        #X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
      "        probas = []\n",
      "        for x in X:\n",
      "            k = np.array([self.W[i]*self.kernel(x, X[i]) for i in xrange(len(X))]).sum()\n",
      "            p = sigmoid(self.bias + k)\n",
      "            sys.stdout.flush()\n",
      "            probas.append([p, 1-p])\n",
      "        return np.array(probas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"kernelized logistic reg: \"\n",
      "for sigma in [0.5, 1, 1.5, 2, 2.5, 3]:\n",
      "    print sigma\n",
      "    do_cv(KLR(sigma=sigma), 5)\n",
      "\n",
      "print \"logistic reg: \"\n",
      "do_cv(LogisticRegression(), 5)\n",
      "\n",
      "print \"GDA, full cov: \"\n",
      "do_cv(GDA(full_cov=True), 5)\n",
      "\n",
      "print \"GDA, diag cov: \"\n",
      "do_cv(GDA(full_cov=False), 5)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kernelized logistic reg: \n",
        "0.5\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train log_loss (mean/std): 5.527219/2.803364\n",
        "train acc (mean/std): 0.762891/0.001806\n",
        "test log_loss (mean/std): 5.477114/2.772613\n",
        "test acc (mean/std): 0.762955/0.007024\n",
        "\n",
        "1\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train log_loss (mean/std): 5.527258/2.803359\n",
        "train acc (mean/std): 0.762891/0.001806\n",
        "test log_loss (mean/std): 5.477094/2.772534\n",
        "test acc (mean/std): 0.762955/0.007024\n",
        "\n",
        "1.5\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train log_loss (mean/std): 5.534090/2.801315\n",
        "train acc (mean/std): 0.762891/0.001806\n",
        "test log_loss (mean/std): 5.478763/2.771629\n",
        "test acc (mean/std): 0.762955/0.007024\n",
        "\n",
        "2\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train log_loss (mean/std): 5.558878/2.793367\n",
        "train acc (mean/std): 0.762891/0.001806\n",
        "test log_loss (mean/std): 5.484610/2.769335\n",
        "test acc (mean/std): 0.762955/0.007024\n",
        "\n",
        "2.5\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train log_loss (mean/std): 5.607784/2.777853\n",
        "train acc (mean/std): 0.762891/0.001806\n",
        "test log_loss (mean/std): 5.496296/2.765229\n",
        "test acc (mean/std): 0.762955/0.007024\n",
        "\n",
        "3\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train log_loss (mean/std): 5.677302/2.756893\n",
        "train acc (mean/std): 0.762891/0.001806\n",
        "test log_loss (mean/std): 5.513052/2.759563\n",
        "test acc (mean/std): 0.762955/0.007024\n",
        "\n",
        "logistic reg: \n",
        "train log_loss (mean/std): 0.417398/0.008902\n",
        "train acc (mean/std): 0.814397/0.009275\n",
        "test log_loss (mean/std): 0.544951/0.030132\n",
        "test acc (mean/std): 0.727058/0.048995\n",
        "\n",
        "GDA, full cov: \n",
        "train log_loss (mean/std): 0.001253/0.002137"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train acc (mean/std): 1.000000/0.000000\n",
        "test log_loss (mean/std): 7.753496/0.534866\n",
        "test acc (mean/std): 0.762955/0.007024\n",
        "\n",
        "GDA, diag cov: \n",
        "train log_loss (mean/std): 1.253732/0.094234"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train acc (mean/std): 0.684371/0.022077\n",
        "test log_loss (mean/std): 1.951759/0.587750\n",
        "test acc (mean/std): 0.622193/0.103874\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}